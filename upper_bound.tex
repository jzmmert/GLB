\section{$\mu$ independent upper bound for Explore then Commit with least square estimator}
\subsection{Problem assumptions}
The following assumptions are currently made, 
\begin{itemize}
\item the arms consist of the surface of the unit ball $\mathcal{B}_1$ (scaling can be absorbed in $\kappa$).
\item we are playing the arms for a total of $T$ time-steps and this value is known beforehand.
\item the norm of $||\theta_*||$ is fixed to 1. (scaling can be absorbed in $\kappa$). We can further use this value in our exploration budget. \color{red}should be generalized to $||\theta_*||\leq 1$ later.\color{black}
\item $0\leq\dot{\mu}\leq \kappa$ \color{red}RHS will be eventually replaced by $\mu(x)\geq \mu(||\theta_*||)-\kappa(1-x)||\theta_*||$\color{black}
\item $\dot{\mu}(x)=\dot{\mu}(-x)$ \color{red}shouldn't be required, but also doesn't hurt much and makes the analysis much easier\color{black}
\item the noise is $\sigma$-subgaussian 
\end{itemize}

We are further defining the following functions
\begin{align}
    L(\theta) &= \EV{(\mu(x^T\theta)-R)^2}\nonumber\\
    &= \EV{(\mu(x^T\theta)-\mu(x^T\theta_*)-\eta)^2}=\EV{(\mu(x^T\theta)-\mu(x^T\theta_*))^2}+\sigma^2\\
    L_n(\theta) &= \frac{1}{n}\sum_{k=1}^n(\mu(x_k^T\theta)-R_k)^2= \frac{1}{n}\sum_{k=1}^n(\mu(x_k^T\theta)-\mu(x_k^T\theta_*)-\eta_k)^2
\end{align}
Obviously $\theta_*$ is the minimum of $L$.\\
The least square estimator is
\begin{align}
    \hat{\theta} := \arg\,\min_{\theta} L_n(\theta)
\end{align}
\textbf{Our algorithm is Explore-then-Commit:}\\
Explore the arms uniformly until a stopping time $\mathcal{T}$ \color{red}currently this is a fixed value because we know the length of $||\theta_*||$ \color{black}. After this commit to playing $\hat{\theta}$ for the remaining time-steps.
\subsection{Results}
\color{red}This is what we aim for, the proof still has gaps though\color{black}
\begin{theorem}
    For any time $T$ and any $\mu$ under the given constraints, with probability at least $1-\delta$, the regret of the given algorithm will be bounded by
    \begin{align}
        \operatorname{Reg}(T) \leq c T\caret\left(\frac{1}{2}+\frac{d-1}{2d+6}\right)\cdot(\kappa\sigma^2\left(\log(T)^2+\log(\delta^{-1})\right))\color{red}^p\color{black}
    \end{align}
\end{theorem}
The proof will require the following lemmas:
\begin{lemma}
    With probability at least $1-\delta$, is holds that 
    \begin{align}
    L(\hat{\theta})-L(\theta_*)\leq \frac{8}{n}\sigma^2(c_1\log(2n)+c_2\log{\delta^{-1}})+\color{red}?\color{black}
    \end{align}
\end{lemma}
\begin{proof}
    As $\hat{\theta}$ is the minimizer of $L_n$, it holds that
    \begin{align*}
        \frac{1}{n}\sum_{k=1}^n(\mu(x_k^T\hat{\theta})-\mu(x_k^T\theta_*))^2 \leq \frac{1}{n}\sum_{k=1}^n(\mu(x_k^T\hat{\theta})-\mu(x_k^T\theta_*))^2 + L_n(\theta_*)-L_n(\hat{\theta})\\
        = \frac{2}{n}\sum_{k=1}^n(\mu(x_k^T\hat{\theta})-\mu(x_k^T\theta_*))\eta_k\\
        = \frac{2}{\sqrt{n}}\sqrt{\frac{1}{n}\sum_{k=1}^n(\mu(x_k^T\hat{\theta})-\mu(x_k^T\theta_*))^2}\sum_{k=1}^n\frac{(\mu(x_k^T\hat{\theta})-\mu(x_k^T\theta_*))}{\sqrt{\sum_{l=1}^n(\mu(x_l^T\hat{\theta})-\mu(x_l^T\theta_*))^2}}\eta_k\\
        w.h.p. \leq   \frac{2}{\sqrt{n}}\sqrt{ \frac{1}{n}\sum_{k=1}^n(\mu(x_k^T\hat{\theta})-\mu(x_k^T\theta_*))^2 }\sigma(c_1\sqrt{\log(2n)}+c_2\sqrt{\log{\delta^{-1}}})\\
        \frac{1}{n}\sum_{k=1}^n(\mu(x_k^T\hat{\theta})-\mu(x_k^T\theta_*))^2 \leq \frac{8}{n}\sigma^2(c_1\log(2n)+c_2\log{\delta^{-1}})
    \end{align*}
    \color{red}TODO: bound $\EV{(\mu(x^T\theta)-\mu(x^T\theta_*))^2}-\frac{1}{n}\sum_{k=1}^n(\mu(x_k^T\hat{\theta})-\mu(x_k^T\theta_*))^2$ for bound on $L(\hat{\theta})-L(\theta_*)$. Write down the constants $c_1, c_2$ explicitly.\color{black}\\
\end{proof}
\begin{lemma}
    Let $\tilde{\theta}$ be the vector $\hat{\theta}$ rescaled such that it matches the length of $\theta_*$. Then 
    $L(\hat{\theta})-L(\theta_*) \geq \frac{1}{2}(L(\tilde{\theta})-L(\theta_*))$
\end{lemma}
\begin{proof}
Let $\theta_1$ and $\theta_2$ be the two orthogonal vectors such that $\theta_* = \theta_1-\theta_2$ and $\tilde{\theta}=\theta_1+\theta_2$.\\
Define the two sets
\begin{align*}
\mathcal{B}_1 &= \left\{x|\sign(x^T\theta_1)=\sign(x^T\theta_2)\right\}\\ 
\mathcal{B}_2 &= \left\{x|\sign(x^T\theta_1)=-\sign(x^T\theta_2)\right\}.
\end{align*}
Due to Symmetry it holds that 
$$\EV{(\mu(x^T\tilde{\theta})-\mu(x^T\theta_*))^2|x\in\mathcal{B}_1}=\EV{(\mu(x^T\tilde{\theta})-\mu(x^T\theta_*))^2|x\in\mathcal{B}_2}$$
If $||\hat{\theta}||_2 > ||\tilde{\theta}||_2$, then $(\mu(x^T\hat{\theta})-\mu(x^T\theta_*))^2 \geq (\mu(x^T\tilde{\theta})-\mu(x^T\theta_*))^2$ for all $x\in \mathcal{B}_1$.\\
If $||\hat{\theta}||_2 < ||\tilde{\theta}||_2$, then $(\mu(x^T\hat{\theta})-\mu(x^T\theta_*))^2 \geq (\mu(x^T\tilde{\theta})-\mu(x^T\theta_*))^2$ for all $x\in \mathcal{B}_2$.\\
Finally we get
\begin{align*}
     &L(\hat{\theta})-L(\theta_*) = \EV{(\mu(x^T\hat{\theta})-\mu(x^T\theta_*))^2}\\
     =& \frac{1}{2}\left(\EV{(\mu(x^T\hat{\theta})-\mu(x^T\theta_*))^2|x\in\mathcal{B}_1}+\EV{(\mu(x^T\hat{\theta})-\mu(x^T\theta_*))^2}|x\in\mathcal{B}_2\right)\\
     \geq& \frac{1}{2}\EV{(\mu(x^T\tilde{\theta})-\mu(x^T\theta_*))^2}=\frac{1}{2}(L(\tilde{\theta})-L(\theta_*))
\end{align*}
\end{proof}
\subsubsection{$d=2$}
\color{red}This is the most important gap. It holds for $\mu = (x-1+\epsilon)_+$, but I haven't succeeded in a general proof.\color{black}
\begin{proposition}
Given any valid $\mu$ with the additional constrain $\int_0^{||\theta_*||}\dot{\mu}(x)\,dx=\epsilon$, then for $d=2$ it holds 
\begin{align*}
    L(\tilde{\theta})-L(\theta_*) \geq \kappa^{-1}(\mu(x_*^T\theta_*)-\mu(\hat{x}^T\theta_*))\epsilon^{-\frac{3}{2}}
\end{align*}
\end{proposition}
\begin{proof}

\begin{align*}
    L(\tilde{\theta})-L(\theta_*)
    =&\EV{(\mu(x^T\theta_1-x^T\theta_2)-\mu(x^T\theta_1+x^T\theta_2))^2}\\
    =&\int_{0}^{2\pi}\left(\mu(\cos(x)|\theta_1|-\sin(x)|\theta_2|)-\mu(\cos(x)|\theta_1|+\sin(x)|\theta_2|)\right)^2\, dx\\
    =&2\int_{0}^{\pi}\left(\int_{-\sin(x)|\theta_2|}^{\sin(x)|\theta_2|}\dot{\mu}(\cos(x)|\theta_1|+y)\,dy\right)^2\, dx\\
    =&2|\theta_2|^2\int_{0}^{\pi}\left(\int_{-1}^{1}\dot{\mu}(\cos(x)|\theta_1|+\sin(x)|\theta_2|y)\,dy\right)^2\sin(x)^2\, dx\\
    =&2|\theta_2|^2\int_{-1}^{1}\left(\int_{-1}^{1}\dot{\mu}(x|\theta_1|+\sqrt{1-x^2}|\theta_2|y)\,dy\right)^2\sqrt{1-x^2}\, dx\\
    \geq &2|\theta_2|^2 \inf_{\beta}\int_{-1}^{1}\left(\int_{-1}^{1}\dot{\mu}(x\sin(\beta)||\theta_*||+\sqrt{1-x^2}\cos(\beta)||\theta_*||y)\,dy\right)^2\sqrt{1-x^2}\, dx\\
    =&:2|\theta_2|^2J(\mu,||\theta_*||)\\
    =&(x_*^T\theta_*-\hat{x}^T\theta_*) J(\mu,||\theta_*||)\\
    \geq&\kappa^{-1}(\mu(x_*^T\theta_*)-\mu(\hat{x}^T\theta_*)) J(\mu,||\theta_*||)\\
\end{align*}
\color{red}TODO: bound $J$\color{black}
\end{proof}
With these lemmas and the proposition, we can proof the theorem for $d=2$.\\
\begin{proof}
    \begin{align*}
        \frac{8}{n}\sigma^2(c_1\log(2n)+c_2\log{\delta^{-1}}) \geq \kappa^{-1}(\mu(x_*^T\theta_*)-\mu(\hat{x}^T\theta_*))\epsilon^{-\frac{3}{2}}\\
        (\mu(x_*^T\theta_*)-\mu(\hat{x}^T\theta_*)) \leq \kappa\epsilon^{\frac{3}{2}}\frac{8}{n}\sigma^2(c_1\log(2n)+c_2\log{\delta^{-1}})
    \end{align*}
    Setting $n= T\caret{\frac{3}{5}}\epsilon^{-1}(8\kappa\sigma^2(c_1\log(2n)+c_2\log{\delta^{-1}}))\caret\frac{2}{5}$, and by using $(\mu(x_*^T\theta_*)-\mu(\hat{x}^T\theta_*)) \leq \epsilon$, we get
    \begin{align*}
        (\mu(x_*^T\theta_*)-\mu(\hat{x}^T\theta_*)) &\leq \min\left\{\epsilon, \epsilon^{\frac{5}{2}}T^{-\frac{3}{5}}(8\kappa\sigma^2(c_1\log(2n)+c_2\log{\delta^{-1}}))^{\frac{3}{5}}\right\}\\
        &\leq T^{-\frac{2}{5}}(8\kappa\sigma^2(c_1\log(2n)+c_2\log{\delta^{-1}}))^{\frac{2}{5}}
    \end{align*}
    The total regret is the sum of exploration and exploration phase, therefore
    \begin{align*}
        \operatorname{Reg}(T) &\leq n\cdot\epsilon + T\cdot(\mu(x_*^T\theta_*)-\mu(\hat{x}^T\theta_*))\\
        &\leq 2 T^{\frac{3}{5}}(8\kappa\sigma^2(c_1\log(2n)+c_2\log{\delta^{-1}}))^{\frac{2}{5}}
    \end{align*}
\end{proof}
\subsection{$d\geq 3$}
We express the Expectation in terms of integrals
\begin{align*}
   & L(\tilde{\theta})-L(\theta_*)\\
    =&\EV{(\mu(x^T\theta_1-x^T\theta_2)-\mu(x^T\theta_1+x^T\theta_2))^2}\\
    =&\frac{1}{|S_d|}\int_{S_d}(\mu(x^T\theta_1-x^T\theta_2)-\mu(x^T\theta_1+x^T\theta_2))^2 \,d\mathcal{L}_{d-1}(x)\\
    =& \frac{2|S_{d-2}|}{|S_d|}\int_{0}^{\frac{\pi}{2}}\cos(\alpha)^{d-2}\\
    &\,\int_{0}^{2\pi}(\mu(\sin(\alpha)(\cos(x)|\theta_1|-\sin(x)|\theta_2|))-\mu(\sin(\alpha)(\cos(x)|\theta_1|+\sin(x)|\theta_2|)))^2 \,dx\,d\alpha\\
    \geq& \frac{4|S_{d-2}|}{|S_d|}\int_{0}^{\frac{\pi}{2}}\cos(\alpha)^{d-2}
    J(\mu,\sin(\alpha)||\theta_*||)\,d\alpha\\
    =& \frac{4|S_{d-2}||\theta_2|^2}{|S_d|}\int_{0}^{1}\frac{\alpha^{d-2}}{\sqrt{1-\alpha^2}}
    J(\mu,\sqrt{1-\alpha^2})||\theta_*||)\,d\alpha\\
\end{align*}