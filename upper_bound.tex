\section{$\mu$ independent upper bound for Explore then Commit with least square estimator}
\subsection{Problem assumptions}
The following assumptions are currently made, 
\begin{itemize}
\item the arms consist of the surface of the unit ball $\mathcal{B}_1$ (scaling can be absorbed in $\kappa$).
\item we are playing the arms for a total of $T$ time-steps and this value is known beforehand.
\item the norm of $||\theta_*||$ is fixed to 1. (scaling can be absorbed in $\kappa$). We can further use this value in our exploration budget. \color{red}should be generalized to $||\theta_*||\leq 1$ later.\color{black}
\item $0\leq\dot{\mu}\leq \kappa$ \color{red}RHS will be eventually replaced by $\mu(x)\geq \mu(||\theta_*||)-\kappa(1-x)||\theta_*||$\color{black}
\item $\dot{\mu}(x)=\dot{\mu}(-x)$ \color{red}shouldn't be required, but also doesn't hurt much and makes the analysis much easier\color{black}
\item the noise is $\sigma$-subgaussian 
\end{itemize}

We are further defining the following functions
\begin{align}
    L(\theta) &= \EV{(\mu(x^T\theta)-R)^2}\nonumber\\
    &= \EV{(\mu(x^T\theta)-\mu(x^T\theta_*)-\eta)^2}=\EV{(\mu(x^T\theta)-\mu(x^T\theta_*))^2}+\sigma^2\\
    L_n(\theta) &= \frac{1}{n}\sum_{k=1}^n(\mu(x_k^T\theta)-R_k)^2= \frac{1}{n}\sum_{k=1}^n(\mu(x_k^T\theta)-\mu(x_k^T\theta_*)-\eta_k)^2
\end{align}
Obviously $\theta_*$ is the minimum of $L$.\\
The least square estimator is
\begin{align}
    \hat{\theta} := \arg\,\min_{\theta} L_n(\theta)
\end{align}
\textbf{Our algorithm is Explore-then-Commit:}\\
Explore the arms uniformly until a stopping time $\mathcal{T}$ \color{red}currently this is a fixed value because we know the length of $||\theta_*||$ \color{black}. After this commit to playing $\hat{\theta}$ for the remaining time-steps.
\subsection{Results}
\color{red}This is what we aim for, the proof still has gaps though\color{black}
\begin{theorem}
    For any time $T$ and any $\mu$ under the given constraints, with probability at least $1-\delta$, the regret of the given algorithm will be bounded by
    \begin{align}
        \operatorname{Reg}(T) \leq c T\caret\left(\frac{1}{2}+\frac{d-1}{2d+6}\right)\cdot(\kappa\sigma^2\left(\log(T)^2+\log(\delta^{-1})\right))\color{red}^p\color{black}
    \end{align}
\end{theorem}
The proof will require the following lemmas:
\begin{lemma}
    With probability at least $1-\delta$, is holds that 
    \begin{align}
    L(\hat{\theta})-L(\theta_*)\leq \frac{8}{n}\sigma^2(c_1\log(2n)+c_2\log{\delta^{-1}})+\color{red}?\color{black}
    \end{align}
\end{lemma}
\begin{proof}
    As $\hat{\theta}$ is the minimizer of $L_n$, it holds that
    \begin{align*}
        \frac{1}{n}\sum_{k=1}^n(\mu(x_k^T\hat{\theta})-\mu(x_k^T\theta_*))^2 \leq \frac{1}{n}\sum_{k=1}^n(\mu(x_k^T\hat{\theta})-\mu(x_k^T\theta_*))^2 + L_n(\theta_*)-L_n(\hat{\theta})\\
        = \frac{2}{n}\sum_{k=1}^n(\mu(x_k^T\hat{\theta})-\mu(x_k^T\theta_*))\eta_k\\
        = \frac{2}{\sqrt{n}}\sqrt{\frac{1}{n}\sum_{k=1}^n(\mu(x_k^T\hat{\theta})-\mu(x_k^T\theta_*))^2}\sum_{k=1}^n\frac{(\mu(x_k^T\hat{\theta})-\mu(x_k^T\theta_*))}{\sqrt{\sum_{l=1}^n(\mu(x_l^T\hat{\theta})-\mu(x_l^T\theta_*))^2}}\eta_k\\
        w.h.p. \leq   \frac{2}{\sqrt{n}}\sqrt{ \frac{1}{n}\sum_{k=1}^n(\mu(x_k^T\hat{\theta})-\mu(x_k^T\theta_*))^2 }\sigma(c_1\sqrt{\log(2n)}+c_2\sqrt{\log{\delta^{-1}}})\\
        \frac{1}{n}\sum_{k=1}^n(\mu(x_k^T\hat{\theta})-\mu(x_k^T\theta_*))^2 \leq \frac{8}{n}\sigma^2(c_1\log(2n)+c_2\log{\delta^{-1}})
    \end{align*}
    \color{red}TODO: bound $\EV{(\mu(x^T\theta)-\mu(x^T\theta_*))^2}-\frac{1}{n}\sum_{k=1}^n(\mu(x_k^T\hat{\theta})-\mu(x_k^T\theta_*))^2$ for bound on $L(\hat{\theta})-L(\theta_*)$. Write down the constants $c_1, c_2$ explicitly.\color{black}\\
\end{proof}
\begin{lemma}
    Let $\tilde{\theta}$ be the vector $\hat{\theta}$ rescaled such that it matches the length of $\theta_*$. Then 
    $L(\hat{\theta})-L(\theta_*) \geq \frac{1}{2}(L(\tilde{\theta})-L(\theta_*))$
\end{lemma}
\begin{proof}
Let $\theta_1$ and $\theta_2$ be the two orthogonal vectors such that $\theta_* = \theta_1-\theta_2$ and $\tilde{\theta}=\theta_1+\theta_2$.\\
Define the two sets
\begin{align*}
\mathcal{B}_1 &= \left\{x|\sign(x^T\theta_1)=\sign(x^T\theta_2)\right\}\\ 
\mathcal{B}_2 &= \left\{x|\sign(x^T\theta_1)=-\sign(x^T\theta_2)\right\}.
\end{align*}
Due to Symmetry it holds that 
$$\EV{(\mu(x^T\tilde{\theta})-\mu(x^T\theta_*))^2|x\in\mathcal{B}_1}=\EV{(\mu(x^T\tilde{\theta})-\mu(x^T\theta_*))^2|x\in\mathcal{B}_2}$$
If $||\hat{\theta}||_2 > ||\tilde{\theta}||_2$, then $(\mu(x^T\hat{\theta})-\mu(x^T\theta_*))^2 \geq (\mu(x^T\tilde{\theta})-\mu(x^T\theta_*))^2$ for all $x\in \mathcal{B}_1$.\\
If $||\hat{\theta}||_2 < ||\tilde{\theta}||_2$, then $(\mu(x^T\hat{\theta})-\mu(x^T\theta_*))^2 \geq (\mu(x^T\tilde{\theta})-\mu(x^T\theta_*))^2$ for all $x\in \mathcal{B}_2$.\\
Finally we get
\begin{align*}
     &L(\hat{\theta})-L(\theta_*) = \EV{(\mu(x^T\hat{\theta})-\mu(x^T\theta_*))^2}\\
     =& \frac{1}{2}\left(\EV{(\mu(x^T\hat{\theta})-\mu(x^T\theta_*))^2|x\in\mathcal{B}_1}+\EV{(\mu(x^T\hat{\theta})-\mu(x^T\theta_*))^2}|x\in\mathcal{B}_2\right)\\
     \geq& \frac{1}{2}\EV{(\mu(x^T\tilde{\theta})-\mu(x^T\theta_*))^2}=\frac{1}{2}(L(\tilde{\theta})-L(\theta_*))
\end{align*}
\end{proof}
\subsubsection{$d=2$}
\color{red}This is the most important gap. It holds for $\mu = (x-1+\epsilon)_+$, but I haven't succeeded in a general proof.\color{black}
\begin{proposition}
Let $d=2$. Then for any valid $\mu \in M$, it holds that 
\begin{align*}
    L(\tilde{\theta})-L(\theta_*) \geq \kappa^{-\frac{1}{2}}(\mu(x_*^T\theta_*)-\mu(\hat{x}^T\theta_*))(\mu(1)-\mu(-1))^{\frac{3}{2}}
\end{align*}
\end{proposition}
The proof follows from the following Lemma
\begin{lemma}
    For any $\mu\in M_1$ and $z \in [0,1]$, it holds that
\begin{align*}
\int_{-1}^1&\left(\mu\left(zx + \iz\ix\right)-\mu\left(zx - \iz\ix\right)\right)^2\frac{1}{\ix}\,dx \\
&\geq c \left(\mu(1)-\mu(z^2)\right)\left(\mu(1)-\mu(-1)\right)^\frac{3}{2}\\
& \Leftrightarrow\\
\int_{-1}^1&(1-z^2)\left(\int_{-1}^1\dot{\mu}\left(zx+\iz\ix y\right)\,dy\right)^2\ix\,dx \\
&\geq c \left(\int_{z^2}^1\dot{\mu}(x)\,dx\right)\left(\int_{-1}^1\dot{\mu}(x)\,dx\right)^\frac{3}{2}
\end{align*}
\end{lemma}
\begin{proof}[Proof of Theorem 6]

\begin{align*}
    L(\tilde{\theta})-L(\theta_*)
    =&\EV{(\mu(x^T\theta_1-x^T\theta_2)-\mu(x^T\theta_1+x^T\theta_2))^2}\\
    =&\int_{0}^{2\pi}\left(\mu(\cos(x)|\theta_1|-\sin(x)|\theta_2|)-\mu(\cos(x)|\theta_1|+\sin(x)|\theta_2|)\right)^2\, dx\\
    =&2\int_{0}^{\pi}\left(\int_{-\sin(x)|\theta_2|}^{\sin(x)|\theta_2|}\dot{\mu}(\cos(x)|\theta_1|+y)\,dy\right)^2\, dx\\
    =&2|\theta_2|^2\int_{0}^{\pi}\left(\int_{-1}^{1}\dot{\mu}(\cos(x)|\theta_1|+\sin(x)|\theta_2|y)\,dy\right)^2\sin(x)^2\, dx\\
    =&2|\theta_2|^2\int_{-1}^{1}\left(\int_{-1}^{1}\dot{\mu}(x|\theta_1|+\sqrt{1-x^2}|\theta_2|y)\,dy\right)^2\sqrt{1-x^2}\, dx\\
...
\end{align*}
\end{proof}

\subsection{$d\geq 3$}
We express the Expectation in terms of integrals
\begin{align*}
   & L(\tilde{\theta})-L(\theta_*)\\
    =&\EV{(\mu(x^T\theta_1-x^T\theta_2)-\mu(x^T\theta_1+x^T\theta_2))^2}\\
    =&\frac{1}{|S_d|}\int_{S_d}(\mu(x^T\theta_1-x^T\theta_2)-\mu(x^T\theta_1+x^T\theta_2))^2 \,d\mathcal{L}_{d-1}(x)\\
    =& \frac{2|S_{d-2}|}{|S_d|}\int_{0}^{\frac{\pi}{2}}\cos(\alpha)^{d-2}\\
    &\,\int_{0}^{2\pi}(\mu(\sin(\alpha)(\cos(x)|\theta_1|-\sin(x)|\theta_2|))-\mu(\sin(\alpha)(\cos(x)|\theta_1|+\sin(x)|\theta_2|)))^2 \,dx\,d\alpha\\
    \geq& \frac{4|S_{d-2}|}{|S_d|}\int_{0}^{\frac{\pi}{2}}\cos(\alpha)^{d-2}
    J(\mu,\sin(\alpha)||\theta_*||)\,d\alpha\\
    =& \frac{4|S_{d-2}||\theta_2|^2}{|S_d|}\int_{0}^{1}\frac{\alpha^{d-2}}{\sqrt{1-\alpha^2}}
    J(\mu,\sqrt{1-\alpha^2})||\theta_*||)\,d\alpha\\
\end{align*}
