\section{$c_{\mu}$ independent lower bound for pseudo maximum-likelihood estimator based algorithms}
We are looking at an explore then exploit strategy with the pseudo maximum likelihood estimator, i.e.
\begin{align}
    \sum_{k=1}^t(\mu(x_k^T\hat{\theta})-\mu(x_k^T\theta_*)-\eta_k)x_k = 0
\end{align}
\begin{theorem}
For any finite time horizon $T$, there exists a generalized linear bandit problem such that after $T$ uniform exploration steps, the regret for playing the optimal arm for $\hat{\theta}$ will be $T^{-\frac{1}{4}}$ with a $T$ independent constant probability.
\end{theorem}
\begin{corollary}
The worst case regret for this algorithm is at least $T^{\frac{3}{4}}$.
\end{corollary}
\begin{proof}
    Define the GLB Problem as follows:\\
    We chose the link function 
    \begin{align}
        \mu(x) := \max\{0, x+\epsilon-1\}
    \end{align}
    The arms lay on the 2d unit ball.\\
    $\theta_*$ is one arm chosen at random.
    After $T$ steps of uniform exploration, we can approximate this for large T with a continuous exploration:
    \begin{align*}
        \sum_{k=1}^T\mu(x_k^T\theta_*)x_k&\approx \frac{T}{2\pi}\int_0^{2\pi} \mu(\cos(x))\cos(x)dx\theta_*\\
        &=\frac{T}{2\pi}\int_{-\cos^{-1}(1-\epsilon)}^{\cos^{-1}(1-\epsilon)} (\cos(x)-1+\epsilon)\cos(x)dx\theta_*\\
        &=\frac{T}{\pi}\left[4\epsilon^{\frac{3}{2}}\sqrt{2-\epsilon}+(2\cos^{-1}(1-\epsilon)+\sin(2\cos^{-1}(1-\epsilon))-4\sin(\cos^{-1}(1-\epsilon))\right]\theta_*\\
        &=\frac{T}{\pi}\left[4\epsilon^{\frac{3}{2}}\sqrt{2-\epsilon} -\mathcal{O}(\cos^{-1}(1-\epsilon)^3)\right]\theta_*
    \end{align*}
    Up to constants and high order terms of the order $\mathcal{O}(\epsilon^\frac{5}{2})$, we have
    \begin{align*}
        \sum_{k=1}^T\mu(x_k^T\theta_*)x_k&\approx T\epsilon^{\frac{3}{2}}\theta_*
    \end{align*}
    Due to symmetry, it also holds that $\sum_{k=1}^T\mu(x_k^T\hat{\theta})x_k$ is collinear to $\hat{\theta}$.\\
    Let $\bar{\theta}$ be orthogonal to $\hat{\theta}$ and of length 1, then
    \begin{align*}
        (\sum_{k=1}^t(\mu(x_k^T\hat{\theta})-\mu(x_k^T\theta_*)-\eta_k)x_k)^T\bar{\theta}=0\\
        T\epsilon^{\frac{3}{2}}\bar{\theta}^T\theta_* \approx \sum_{k=1}^T\eta_k x_k^T\bar{\theta}
    \end{align*}
    We assume the noise is gaussian with variance 1, then the RHS is a gaussian random variable with variance approximately
    \begin{align*}
        \mathbb{V}\left[\sum_{k=1}^T\eta_k x_k^T\bar{\theta}\right] \approx \frac{T}{\pi}\int_0^\pi \cos^2(x)dx =\frac{T}{2}
    \end{align*}
    With a constant probability, it therefore holds up to constants
    \begin{align*}
        \bar{\theta}^T\theta_* \geq \sqrt{\epsilon^{-3}T^{-1}}\\
    \end{align*}
    Setting $\epsilon= T^{\frac{-1}{4}}$, we have up to constants
    \begin{align*}
        \bar{\theta}^T\theta_* \geq \sqrt{\epsilon} \approx \sqrt{\epsilon(2-\epsilon)}\\
    \hat{x}^T\theta_* = \sqrt{1-(\bar{\theta}^T\theta_*)^2} \leq 1-\epsilon
    \end{align*}
    The immediate regret after exploration is therefore at least $\epsilon =T^{-\frac{1}{4}}$
    
\end{proof}
